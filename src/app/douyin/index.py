import json
import os
import yaml
import httpx
from src.crawlers.base_crawler import BaseCrawler
from src.crawlers.douyin.endpoints import DouyinAPIEndpoints
from src.crawlers.douyin.util import AwemeIdFetcher, BogusManager
from src.crawlers.util import PostDetail
from src.crawlers.exceptions import APIResponseError
from src.utils import get_analyze_logger, config
from src.utils.index import find_url
from src.utils.response import Response
from urllib.parse import urlencode
from pathlib import Path


logger = get_analyze_logger()

# é…ç½®æ–‡ä»¶è·¯å¾„
# Read the configuration file
path = Path(__file__).parent.parent.parent / "crawlers" / "douyin" / "config.yaml"

# è¯»å–é…ç½®æ–‡ä»¶
with open(f"{path}", "r", encoding="utf-8") as f:
    douyinConfig = yaml.safe_load(f)
    logger.info(f"douyinConfig: {douyinConfig}")


class Douyin:
    def __init__(self, text, type):
        self.url = find_url(text)
        self.text = text
        self.type = type
        self.aweme_id = None
        self.video_data = None
        # åˆå§‹åŒ–æ—¶ä¸æ‰§è¡Œå¼‚æ­¥æ“ä½œï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶è°ƒç”¨

    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ–¹æ³•"""
        try:
            self.aweme_id = await AwemeIdFetcher.get_aweme_id(self.url)
            logger.info(f"aweme_id: {self.aweme_id}")
            self.video_data = await self.fetch_one_video(self.aweme_id)
            logger.info(f"video_data: {self.video_data}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æŠ–éŸ³æ•°æ®æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            raise

    # ä»é…ç½®æ–‡ä»¶ä¸­è·å–æŠ–éŸ³çš„è¯·æ±‚å¤´
    async def get_douyin_headers(self):
        douyin_config = douyinConfig["TokenManager"]["douyin"]
        kwargs = {
            "headers": {
                "Accept-Language": douyin_config["headers"]["Accept-Language"],
                "User-Agent": douyin_config["headers"]["User-Agent"],
                "Referer": douyin_config["headers"]["Referer"],
                "Cookie": douyin_config["headers"]["Cookie"],
            },
            "proxies": {
                "http://": douyin_config["proxies"]["http"],
                "https://": douyin_config["proxies"]["https"],
            },
        }
        return kwargs

     # è·å–å•ä¸ªä½œå“æ•°æ®
    async def fetch_one_video(self, aweme_id: str):
        # å¯¼å…¥å¿…è¦æ¨¡å—
        import asyncio
        from src.crawlers.exceptions import APIResponseError

        # å°è¯•å¤šç§ç­–ç•¥è·å–æ•°æ®
        strategies = [
            self._strategy_web_api,
            self._strategy_bypass_detection,
            self._strategy_mobile_api,
            self._strategy_alternative_endpoint,
            self._strategy_emergency_fallback
        ]

        for i, strategy in enumerate(strategies, 1):
            try:
                logger.info(f"ğŸ”„ å°è¯•ç­–ç•¥ {i}/{len(strategies)}: {strategy.__name__}")
                result = await strategy(aweme_id)
                if result and self._is_valid_response(result):
                    logger.info(f"âœ… ç­–ç•¥ {i} æˆåŠŸè·å–æ•°æ®")
                    return result
                else:
                    logger.warning(f"âŒ ç­–ç•¥ {i} è¿”å›æ— æ•ˆæ•°æ®")
            except Exception as e:
                logger.error(f"âŒ ç­–ç•¥ {i} æ‰§è¡Œå¤±è´¥: {str(e)}")

            # ç­–ç•¥é—´å»¶è¿Ÿ
            if i < len(strategies):
                await asyncio.sleep(2)

        logger.error("ğŸš« æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†")
        raise APIResponseError("æ‰€æœ‰è·å–ç­–ç•¥éƒ½å¤±è´¥")

    def _is_valid_response(self, response):
        """éªŒè¯å“åº”æ˜¯å¦æœ‰æ•ˆ"""
        if not response:
            return False
        if isinstance(response, dict):
            return len(str(response)) > 100  # ç®€å•æ£€æŸ¥æ•°æ®é‡
        return True

    async def _strategy_web_api(self, aweme_id: str):
        """ç­–ç•¥1: æ ‡å‡†Web API"""
        import asyncio
        from src.crawlers.douyin.anti_detection import AntiDetectionManager, CookieManager

        # è·å–æŠ–éŸ³çš„å®æ—¶Cookie
        kwargs = await self.get_douyin_headers()

        # éªŒè¯Cookieæœ‰æ•ˆæ€§
        if not CookieManager.validate_cookie_freshness(kwargs["headers"].get("Cookie", "")):
            logger.warning("Cookieå¯èƒ½å·²è¿‡æœŸï¼Œå»ºè®®æ›´æ–°")

        # ä½¿ç”¨åæ£€æµ‹ç®¡ç†å™¨ç”ŸæˆçœŸå®çš„å‚æ•°
        base_params = PostDetail(aweme_id=aweme_id).model_dump()
        realistic_params = AntiDetectionManager.generate_realistic_params(base_params)

        params = PostDetail(aweme_id=aweme_id, **{k: v for k, v in realistic_params.items() if k != 'aweme_id'})
        params_dict = params.model_dump()

        # ç”Ÿæˆa_bogusç­¾å
        a_bogus = BogusManager.ab_model_2_endpoint(params_dict, kwargs["headers"]["User-Agent"])

        # æ„å»ºå®Œæ•´çš„è¯·æ±‚URL
        endpoint = f"{DouyinAPIEndpoints.POST_DETAIL}?{urlencode(params_dict)}&a_bogus={a_bogus}"

        logger.info("=" * 80)
        logger.info("æŠ–éŸ³Web APIè¯·æ±‚è¯¦ç»†ä¿¡æ¯:")
        logger.info(f"å®Œæ•´è¯·æ±‚URL: {endpoint}")
        logger.info(f"a_bogusç­¾å: {a_bogus}")
        logger.info("=" * 80)

        # ä½¿ç”¨åæ£€æµ‹ç®¡ç†å™¨æ·»åŠ æ™ºèƒ½å»¶è¿Ÿ
        delay = AntiDetectionManager.add_timing_jitter()
        await asyncio.sleep(delay)

        # ä½¿ç”¨åæ£€æµ‹ç®¡ç†å™¨ç”ŸæˆçœŸå®çš„è¯·æ±‚å¤´
        enhanced_headers = AntiDetectionManager.generate_realistic_headers(kwargs["headers"])

        # åˆ›å»ºä¸€ä¸ªåŸºç¡€çˆ¬è™«
        base_crawler = BaseCrawler(proxies=kwargs["proxies"], crawler_headers=enhanced_headers)
        async with base_crawler as crawler:
            response = await crawler.fetch_get_json(endpoint)
            return response

    async def _strategy_mobile_api(self, aweme_id: str):
        """ç­–ç•¥2: ç§»åŠ¨ç«¯APIæ¨¡æ‹Ÿ"""
        import asyncio
        import time

        logger.info("ğŸ”„ å°è¯•ç§»åŠ¨ç«¯APIç­–ç•¥")

        # è·å–ç§»åŠ¨ç«¯é…ç½®
        kwargs = await self.get_douyin_headers()

        # ä¿®æ”¹ä¸ºç§»åŠ¨ç«¯å‚æ•°
        mobile_params = {
            'aweme_id': aweme_id,
            'device_platform': 'android',
            'aid': '1128',
            'version_code': '290100',
            'version_name': '29.1.0',
            'device_type': 'SM-G973F',
            'os_version': '10',
            'resolution': '1080*2340',
            'dpi': '480',
            'update_version_code': '290100',
            'ac': 'wifi',
            'channel': 'googleplay',
            'app_name': 'aweme',
            'version_code': '290100',
            'manifest_version_code': '290100',
            'app_type': 'normal'
        }

        # ç§»åŠ¨ç«¯User-Agent
        mobile_headers = kwargs["headers"].copy()
        mobile_headers.update({
            'User-Agent': 'com.ss.android.ugc.aweme/290100 (Linux; U; Android 10; zh_CN; SM-G973F; Build/QP1A.190711.020; Cronet/TTNetVersion:b4d74d15 2020-04-23 QuicVersion:0144d358 2020-03-24)',
            'X-Khronos': str(int(time.time())),
            'X-Gorgon': self._generate_gorgon(),
            'X-Ladon': self._generate_ladon(),
        })

        # æ„å»ºç§»åŠ¨ç«¯API URL
        mobile_endpoint = f"https://aweme.snssdk.com/aweme/v1/aweme/detail/?{urlencode(mobile_params)}"

        logger.info(f"ç§»åŠ¨ç«¯API URL: {mobile_endpoint}")

        await asyncio.sleep(1)

        base_crawler = BaseCrawler(proxies=kwargs["proxies"], crawler_headers=mobile_headers)
        async with base_crawler as crawler:
            response = await crawler.fetch_get_json(mobile_endpoint)
            return response

    async def _strategy_alternative_endpoint(self, aweme_id: str):
        """ç­–ç•¥3: å¤‡ç”¨ç«¯ç‚¹"""
        import asyncio

        logger.info("ğŸ”„ å°è¯•å¤‡ç”¨ç«¯ç‚¹ç­–ç•¥")

        kwargs = await self.get_douyin_headers()

        # å°è¯•ä¸åŒçš„ç«¯ç‚¹
        alternative_endpoints = [
            f"https://www.douyin.com/aweme/v1/web/aweme/detail/?aweme_id={aweme_id}&aid=1128&version_name=23.5.0&device_platform=webapp&os=pc",
            f"https://www.iesdouyin.com/web/api/v2/aweme/iteminfo/?item_ids={aweme_id}",
            f"https://www.douyin.com/aweme/v1/web/aweme/post/?aweme_id={aweme_id}"
        ]

        for endpoint in alternative_endpoints:
            try:
                logger.info(f"å°è¯•ç«¯ç‚¹: {endpoint}")

                # ç®€åŒ–çš„è¯·æ±‚å¤´
                simple_headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                    'Referer': 'https://www.douyin.com/',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Cookie': kwargs["headers"].get("Cookie", "")
                }

                await asyncio.sleep(1)

                base_crawler = BaseCrawler(proxies=kwargs["proxies"], crawler_headers=simple_headers)
                async with base_crawler as crawler:
                    response = await crawler.fetch_get_json(endpoint)
                    if response and len(str(response)) > 50:
                        logger.info(f"âœ… å¤‡ç”¨ç«¯ç‚¹æˆåŠŸ: {endpoint}")
                        return response

            except Exception as e:
                logger.warning(f"å¤‡ç”¨ç«¯ç‚¹å¤±è´¥ {endpoint}: {str(e)}")
                continue

        return None

    async def _strategy_bypass_detection(self, aweme_id: str):
        """ç­–ç•¥2: æ£€æµ‹ç»•è¿‡"""
        from src.crawlers.douyin.bypass_detection import DouyinBypassManager

        logger.info("ğŸ”„ å°è¯•æ£€æµ‹ç»•è¿‡ç­–ç•¥")

        kwargs = await self.get_douyin_headers()

        # å°è¯•Webç«¯ç»•è¿‡
        result = await DouyinBypassManager.bypass_web_detection(aweme_id, kwargs["headers"])
        if result:
            return result

        # å°è¯•ç§»åŠ¨ç«¯ç»•è¿‡
        result = await DouyinBypassManager.bypass_mobile_detection(aweme_id)
        if result:
            return result

        return None

    async def _strategy_emergency_fallback(self, aweme_id: str):
        """ç­–ç•¥5: ç´§æ€¥å¤‡ç”¨æ–¹æ¡ˆ"""
        from src.crawlers.douyin.bypass_detection import DouyinBypassManager

        logger.info("ğŸ†˜ å¯ç”¨ç´§æ€¥å¤‡ç”¨ç­–ç•¥")

        # ä½¿ç”¨ç´§æ€¥å¤‡ç”¨æ–¹æ¡ˆ
        result = await DouyinBypassManager.emergency_fallback(aweme_id)
        return result

    def _generate_gorgon(self):
        """ç”ŸæˆX-Gorgonå¤´"""
        import hashlib
        import time
        timestamp = str(int(time.time()))
        return hashlib.md5(f"gorgon_{timestamp}".encode()).hexdigest()[:8]

    def _generate_ladon(self):
        """ç”ŸæˆX-Ladonå¤´"""
        import hashlib
        import time
        timestamp = str(int(time.time()))
        return hashlib.md5(f"ladon_{timestamp}".encode()).hexdigest()[:8]

    def to_dict(self):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼Œç”¨äº API è¿”å›"""
        try:
            result = {
                "aweme_id": self.aweme_id,
                "video_data": self.video_data,
                # "url": self.url,
                # "final_url": "",
                # "title": self.title,
                # "description": self.description,
                # "image_list": self.image_list,
                # "video": self.video,
                # "app_type": "douyin",
            }
            return Response.success(result, "è·å–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æŠ–éŸ³è½¬æ¢ä¸ºå­—å…¸æ—¶å‡ºé”™: {str(e)}", exc_info=True)
            return Response.error("è·å–å¤±è´¥")
