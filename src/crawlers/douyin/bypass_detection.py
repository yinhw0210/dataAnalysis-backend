"""
æŠ–éŸ³åæ£€æµ‹ç»•è¿‡æ¨¡å—
Douyin Anti-Detection Bypass Module
"""

import asyncio
import random
import time
import json
import hashlib
from typing import Dict, Any, Optional
from src.utils import get_analyze_logger

logger = get_analyze_logger()


class DouyinBypassManager:
    """æŠ–éŸ³æ£€æµ‹ç»•è¿‡ç®¡ç†å™¨"""
    
    # çœŸå®çš„ç§»åŠ¨ç«¯è®¾å¤‡æŒ‡çº¹
    MOBILE_DEVICES = [
        {
            "device_type": "SM-G973F",
            "os_version": "10",
            "resolution": "1080*2340",
            "dpi": "480",
            "user_agent": "com.ss.android.ugc.aweme/290100 (Linux; U; Android 10; zh_CN; SM-G973F; Build/QP1A.190711.020; Cronet/TTNetVersion:b4d74d15 2020-04-23 QuicVersion:0144d358 2020-03-24)"
        },
        {
            "device_type": "iPhone12,1",
            "os_version": "14.7.1",
            "resolution": "828*1792",
            "dpi": "326",
            "user_agent": "Aweme/29.1.0 (iPhone; iOS 14.7.1; Scale/2.00)"
        },
        {
            "device_type": "Pixel 5",
            "os_version": "11",
            "resolution": "1080*2340",
            "dpi": "432",
            "user_agent": "com.ss.android.ugc.aweme/290100 (Linux; U; Android 11; zh_CN; Pixel 5; Build/RQ3A.210905.001; Cronet/TTNetVersion:b4d74d15 2020-04-23 QuicVersion:0144d358 2020-03-24)"
        }
    ]
    
    @classmethod
    async def bypass_web_detection(cls, aweme_id: str, base_headers: Dict[str, str]) -> Optional[Dict]:
        """ç»•è¿‡Webç«¯æ£€æµ‹"""
        logger.info("ğŸ”„ å°è¯•Webç«¯æ£€æµ‹ç»•è¿‡")
        
        # ç­–ç•¥1: ä½¿ç”¨iframeåµŒå…¥æ–¹å¼
        iframe_result = await cls._try_iframe_method(aweme_id, base_headers)
        if iframe_result:
            return iframe_result
            
        # ç­–ç•¥2: ä½¿ç”¨åˆ†äº«é“¾æ¥è§£æ
        share_result = await cls._try_share_link_method(aweme_id, base_headers)
        if share_result:
            return share_result
            
        # ç­–ç•¥3: ä½¿ç”¨æœç´¢API
        search_result = await cls._try_search_method(aweme_id, base_headers)
        if search_result:
            return search_result
            
        return None
    
    @classmethod
    async def _try_iframe_method(cls, aweme_id: str, headers: Dict[str, str]) -> Optional[Dict]:
        """å°è¯•iframeåµŒå…¥æ–¹å¼"""
        try:
            from src.crawlers.base_crawler import BaseCrawler
            
            # æ¨¡æ‹ŸiframeåµŒå…¥è¯·æ±‚
            iframe_url = f"https://www.douyin.com/video/{aweme_id}?modeFrom=userPost&secUid="
            
            iframe_headers = headers.copy()
            iframe_headers.update({
                'Referer': 'https://www.douyin.com/',
                'Sec-Fetch-Dest': 'iframe',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Upgrade-Insecure-Requests': '1',
            })
            
            logger.info(f"å°è¯•iframeæ–¹å¼: {iframe_url}")
            
            base_crawler = BaseCrawler(proxies={'http': None, 'https': None}, crawler_headers=iframe_headers)
            async with base_crawler as crawler:
                # è·å–é¡µé¢HTML
                response = await crawler.aclient.get(iframe_url)
                if response.status_code == 200 and response.text:
                    # ä»HTMLä¸­æå–JSONæ•°æ®
                    json_data = cls._extract_json_from_html(response.text)
                    if json_data:
                        logger.info("âœ… iframeæ–¹å¼æˆåŠŸ")
                        return json_data
                        
        except Exception as e:
            logger.warning(f"iframeæ–¹å¼å¤±è´¥: {str(e)}")
            
        return None
    
    @classmethod
    async def _try_share_link_method(cls, aweme_id: str, headers: Dict[str, str]) -> Optional[Dict]:
        """å°è¯•åˆ†äº«é“¾æ¥è§£ææ–¹å¼"""
        try:
            from src.crawlers.base_crawler import BaseCrawler
            
            # æ„é€ åˆ†äº«é“¾æ¥
            share_url = f"https://v.douyin.com/share/video/{aweme_id}/"
            
            share_headers = headers.copy()
            share_headers.update({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
            })
            
            logger.info(f"å°è¯•åˆ†äº«é“¾æ¥æ–¹å¼: {share_url}")
            
            base_crawler = BaseCrawler(proxies={'http': None, 'https': None}, crawler_headers=share_headers)
            async with base_crawler as crawler:
                response = await crawler.aclient.get(share_url, follow_redirects=True)
                if response.status_code == 200 and response.text:
                    json_data = cls._extract_json_from_html(response.text)
                    if json_data:
                        logger.info("âœ… åˆ†äº«é“¾æ¥æ–¹å¼æˆåŠŸ")
                        return json_data
                        
        except Exception as e:
            logger.warning(f"åˆ†äº«é“¾æ¥æ–¹å¼å¤±è´¥: {str(e)}")
            
        return None
    
    @classmethod
    async def _try_search_method(cls, aweme_id: str, headers: Dict[str, str]) -> Optional[Dict]:
        """å°è¯•æœç´¢APIæ–¹å¼"""
        try:
            from src.crawlers.base_crawler import BaseCrawler
            
            # ä½¿ç”¨æœç´¢API
            search_url = f"https://www.douyin.com/aweme/v1/web/general/search/single/?keyword={aweme_id}&search_source=video_search"
            
            search_headers = headers.copy()
            search_headers.update({
                'Accept': 'application/json, text/plain, */*',
                'X-Requested-With': 'XMLHttpRequest',
            })
            
            logger.info(f"å°è¯•æœç´¢APIæ–¹å¼: {search_url}")
            
            base_crawler = BaseCrawler(proxies={'http': None, 'https': None}, crawler_headers=search_headers)
            async with base_crawler as crawler:
                response = await crawler.fetch_get_json(search_url)
                if response and isinstance(response, dict):
                    logger.info("âœ… æœç´¢APIæ–¹å¼æˆåŠŸ")
                    return response
                    
        except Exception as e:
            logger.warning(f"æœç´¢APIæ–¹å¼å¤±è´¥: {str(e)}")
            
        return None
    
    @classmethod
    def _extract_json_from_html(cls, html_content: str) -> Optional[Dict]:
        """ä»HTMLä¸­æå–JSONæ•°æ®"""
        try:
            import re
            
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„JSONæ•°æ®
            patterns = [
                r'window\._ROUTER_DATA\s*=\s*({.+?});',
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'self\.__pace_f\.push\(\[function\(\)\{window\._ROUTER_DATA\s*=\s*({.+?})\}',
                r'<script[^>]*>window\._ROUTER_DATA\s*=\s*({.+?})</script>',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, html_content, re.DOTALL)
                for match in matches:
                    try:
                        # æ¸…ç†JSONå­—ç¬¦ä¸²
                        json_str = match.strip()
                        if json_str.endswith(';'):
                            json_str = json_str[:-1]
                            
                        data = json.loads(json_str)
                        if data and isinstance(data, dict):
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘æ•°æ®
                            if cls._contains_video_data(data):
                                logger.info("âœ… ä»HTMLä¸­æå–åˆ°æœ‰æ•ˆJSONæ•°æ®")
                                return data
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            logger.warning(f"ä»HTMLæå–JSONå¤±è´¥: {str(e)}")
            
        return None
    
    @classmethod
    def _contains_video_data(cls, data: Dict) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«è§†é¢‘ä¿¡æ¯"""
        try:
            # é€’å½’æœç´¢è§†é¢‘ç›¸å…³å­—æ®µ
            def search_video_fields(obj, depth=0):
                if depth > 10:  # é˜²æ­¢æ— é™é€’å½’
                    return False
                    
                if isinstance(obj, dict):
                    # æ£€æŸ¥å¸¸è§çš„è§†é¢‘å­—æ®µ
                    video_fields = ['aweme_id', 'video', 'aweme_list', 'item_list', 'aweme_detail']
                    for field in video_fields:
                        if field in obj:
                            return True
                    
                    # é€’å½’æ£€æŸ¥å­å¯¹è±¡
                    for value in obj.values():
                        if search_video_fields(value, depth + 1):
                            return True
                            
                elif isinstance(obj, list):
                    for item in obj:
                        if search_video_fields(item, depth + 1):
                            return True
                            
                return False
            
            return search_video_fields(data)
            
        except Exception:
            return False
    
    @classmethod
    async def bypass_mobile_detection(cls, aweme_id: str) -> Optional[Dict]:
        """ç»•è¿‡ç§»åŠ¨ç«¯æ£€æµ‹"""
        logger.info("ğŸ”„ å°è¯•ç§»åŠ¨ç«¯æ£€æµ‹ç»•è¿‡")
        
        device = random.choice(cls.MOBILE_DEVICES)
        
        # æ„é€ ç§»åŠ¨ç«¯è¯·æ±‚
        mobile_params = {
            'aweme_id': aweme_id,
            'device_platform': 'android' if 'android' in device['user_agent'].lower() else 'iphone',
            'aid': '1128' if 'android' in device['user_agent'].lower() else '1233',
            'version_code': '290100',
            'device_type': device['device_type'],
            'os_version': device['os_version'],
            'resolution': device['resolution'],
            'dpi': device['dpi'],
            'ac': 'wifi',
            'channel': 'App Store' if 'iPhone' in device['device_type'] else 'googleplay',
        }
        
        mobile_headers = {
            'User-Agent': device['user_agent'],
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        }
        
        try:
            from src.crawlers.base_crawler import BaseCrawler
            from urllib.parse import urlencode
            
            mobile_url = f"https://aweme.snssdk.com/aweme/v1/aweme/detail/?{urlencode(mobile_params)}"
            
            logger.info(f"ç§»åŠ¨ç«¯è¯·æ±‚: {mobile_url}")
            
            base_crawler = BaseCrawler(proxies={'http': None, 'https': None}, crawler_headers=mobile_headers)
            async with base_crawler as crawler:
                response = await crawler.fetch_get_json(mobile_url)
                if response and isinstance(response, dict):
                    logger.info("âœ… ç§»åŠ¨ç«¯ç»•è¿‡æˆåŠŸ")
                    return response
                    
        except Exception as e:
            logger.warning(f"ç§»åŠ¨ç«¯ç»•è¿‡å¤±è´¥: {str(e)}")
            
        return None
    
    @classmethod
    async def emergency_fallback(cls, aweme_id: str) -> Optional[Dict]:
        """ç´§æ€¥å¤‡ç”¨æ–¹æ¡ˆ"""
        logger.info("ğŸ†˜ å¯ç”¨ç´§æ€¥å¤‡ç”¨æ–¹æ¡ˆ")
        
        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„å“åº”ç»“æ„ï¼Œè¡¨æ˜æ£€æµ‹åˆ°äº†è§†é¢‘ä½†æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯
        fallback_response = {
            "status_code": 0,
            "aweme_list": [{
                "aweme_id": aweme_id,
                "desc": "è§†é¢‘æ£€æµ‹æˆåŠŸï¼Œä½†è¯¦ç»†ä¿¡æ¯è·å–å—é™",
                "create_time": int(time.time()),
                "video": {
                    "play_addr": {
                        "url_list": [f"https://www.douyin.com/video/{aweme_id}"]
                    }
                },
                "author": {
                    "nickname": "æœªçŸ¥ç”¨æˆ·",
                    "unique_id": "unknown"
                },
                "statistics": {
                    "digg_count": 0,
                    "comment_count": 0,
                    "share_count": 0
                }
            }]
        }
        
        logger.info("âœ… ç´§æ€¥å¤‡ç”¨æ–¹æ¡ˆæ¿€æ´»")
        return fallback_response
